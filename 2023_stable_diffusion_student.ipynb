{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9dvbw-2WEQQ"
      },
      "source": [
        "# Lab: Diffusion Models with ðŸ§¨ Diffusers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3m7dfv6WEQR"
      },
      "source": [
        "**Author:**\n",
        "- baptiste.engel@cea.fr\n",
        "\n",
        "If you have questions or suggestions, contact us and we will gladly answer and take into account your remarks.\n",
        "\n",
        "**Acknowledgement**  \n",
        "This lab is partially inspired by ðŸ§¨ Diffusers's [stable_diffusion.ipynb](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=-xMJ6LaET6dT) notebook. Feel free to explore this notebook also!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0 - Introduction\n"
      ],
      "metadata": {
        "id": "QkwAGX8yaShN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lGzQG59WEQU"
      },
      "source": [
        "Stable Diffusion is an open-source diffusion model trained on [LAION-5B](https://laion.ai/blog/laion-5b/), a 5 billion image-text pairs dataset.\n",
        "\n",
        "This lab guides you through the implementation of your own local Stable Diffusion using the ðŸ§¨ Diffusers library from ðŸ¤— Hugging Face. You will first learn how to setup a complete pipeline to perform inference on a pre-trained Stable Diffusion model, then fine-tune it on your own data with Low Rank Adaptation (LoRA).\n",
        "\n",
        "![baguette_cat](https://raw.githubusercontent.com/engelba/generative/main/tp/assets/baguette_cat.jpg)  \n",
        "*The baguette cat from LAION*\n",
        "\n",
        "\n",
        "You're encouraged to try various things, play with the parameters of the provided function. Feel free to explore the possibility of diffusion models to better understand how they work.  \n",
        "\n",
        "Notice that training a Stable Diffusion model *from scratch* needs around 25 days of compute with 256 A100 GPUs (~20k â‚¬ per unit). The market cost of such a training is around 600k â‚¬: you won't do it yourself on this lab.\n",
        "\n",
        "### Free Choice\n",
        "\n",
        "Stable Diffusion is only one of the many model that come with ðŸ§¨ Diffusers. Eg., if you rather want to implement a text-to-music model, you can e.g. explore [MusicLDM](https://huggingface.co/docs/diffusers/api/pipelines/musicldm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7-EfnujWEQS"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSVXQT98WEQS"
      },
      "source": [
        "This lab will be easier if you use a colab environment to run it. Ensure that you have a GPU environment running (indicated by 'T4' near RAM and Disk at the top right of the notebook).\n",
        "\n",
        "CUDA is an interface that facilitate computing on GPU. Ensure that it is available by running next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrLgEvomWEQS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a helper function to display a grid of images, that will be useful later"
      ],
      "metadata": {
        "id": "kfdGJVPMej6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    \"\"\"\n",
        "    From https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\n",
        "    \"\"\"\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ],
      "metadata": {
        "id": "5VFTXGQYehnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFg6VyJIWEQT"
      },
      "source": [
        "Now, install the useful packages by running the next cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILQRYfhpWEQT"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.23.0\n",
        "!pip install diffusers[training]==0.23.0\n",
        "!pip install transformers scipy ftfy accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stable Diffusion"
      ],
      "metadata": {
        "id": "Q6W5L4nHageQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKbF7cePWEQU"
      },
      "source": [
        "Stable Diffusion (SD) is a text-to-image diffusion model, meaning that from a textual prompt, it outputs an image that must match.\n",
        "\n",
        "The architecture of Stable Diffusion is described in [High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752). Specifically, SD is a latent diffusion model: the denoising diffusion process is done in the latent space of an autoencoder. For noise estimation, model uses an an improved [UNet](https://arxiv.org/abs/1505.04597).\n",
        "\n",
        "\n",
        "![Stable Diffusion](https://miro.medium.com/v2/resize:fit:4800/format:webp/0*rW_y1kjruoT9BSO0.png)\n",
        "\n",
        "SD is thus composed of 3 essential components:\n",
        "\n",
        "**1) The autoencoder (VAE)**\n",
        "\n",
        "The autoencoder is used to encode the image in a latent space, so that the denoising diffusion process requires less compute, even for large image generation. It is trained beforehand, and frozen during training of the diffusion model.\n",
        "\n",
        "**2) The UNet**\n",
        "\n",
        "The UNet in SD is composed of ResNet blocks. Shortcut connections are also made between the encoder and the decoder blocks, to minimize the information loss.\n",
        "\n",
        "Furthermore, for conditionning of the denoising process, cross-attention blocks are added in both the encoder and the decoder, usually after the ResNet blocks.\n",
        "\n",
        "**3) The text encoder**\n",
        "\n",
        "The role of the text encoder is to transform prompt into embeddings, so that it can be processed by the UNet.\n",
        "\n",
        "Text embedding is a complex task, and SD delegates it to a frozen [CLIP](https://github.com/openai/CLIP) ViT-L/14.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ§¨ Diffusers\n"
      ],
      "metadata": {
        "id": "JTY3O8HHankq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§¨ [Diffusers](https://huggingface.co/docs/diffusers/index) is a modular\n",
        "library for performing inference or even training of state-of-the-art diffusion models. It comes with out-of-the-box diffusion models but offers the possibility to split the model into their smaller components.\n",
        "\n",
        "It implements many diffusion models, and can easily be used to perform inference on open-source diffusion models. It also allows you to implement and test your own research.\n",
        "\n",
        "\n",
        "The documentation can be found [here](https://huggingface.co/docs/diffusers/index)."
      ],
      "metadata": {
        "id": "ORdMSNYkatIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Stable Diffusion Implementation with diffusers\n"
      ],
      "metadata": {
        "id": "39vM2mhUaXY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first begin to play with the complete Stable Diffusion 1.5 pipeline. You can start by running the following cell:"
      ],
      "metadata": {
        "id": "5w1tSYFqcoW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "repo_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)"
      ],
      "metadata": {
        "id": "i4Ge6EnlcJyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"pipeline\" object bundles all the components of the diffusion model you just load, and handle most of the work for you.\n",
        "\n",
        "Use the [reference documentation](https://huggingface.co/docs/diffusers/v0.23.0/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline) to display the different components of the pipeline"
      ],
      "metadata": {
        "id": "2H3vwJ9PnQGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Print the VAE configuration:\n",
        "\n",
        "# CODE: Print the UNet configuration\n",
        "\n",
        "# CODE: Print the text encoder configuration:\n"
      ],
      "metadata": {
        "id": "6n-o96gUnO-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that your model is loaded, you can use it to generate images. Remember to move your pipeline to the CUDA device to benefit from hardware acceleration!\n",
        "\n",
        "When running the freshly downloaded pipeline, you will notice that **only 50 steps** of denoising are performed. This is because StableDiffusionPipeline uses the [PNDMScheduler](https://huggingface.co/docs/diffusers/v0.23.1/en/api/schedulers/pndm#diffusers.PNDMScheduler), a more efficient sampling method than the standard DDIM.\n",
        "\n",
        "If you want inspiration for prompts, you can check https://stablediffusion.fr/prompts"
      ],
      "metadata": {
        "id": "9jdkdEbtR7UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Move the pipe to the CUDA device\n",
        "\n",
        "# CODE: Choose a prompt and run the pipeline\n",
        "\n",
        "# CODE: Display the generated image\n"
      ],
      "metadata": {
        "id": "niWU7cYPR6bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also experiment with various parameters of the pipeline:\n",
        "- `height` and `width` allow you to control the size of the generated image.\n",
        "- `num_inference_steps` is the number of denoising steps to generate the image. Try varying it between 1 and 1000. What trade-offs do you observe?\n",
        "- `guidance_scale` controls how strongly the prompt should be followed. Experiment with a classifier guidance of 0, and then with a high number. What happens?\n",
        "- `negative_prompt` is a prompt that instructs the model on what not to do. Try generating a scene with people in it. Then, using the same prompt and seed, alter the scene with negative prompting.\n",
        "\n",
        "\n",
        "You can find all the parameters in the StableDiffusionPipeline [documentation](https://huggingface.co/docs/diffusers/v0.23.1/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline).\n",
        "\n",
        "Tips: to compare your results, pass a torch Generator to your pipeline (`generator` argument). Initialize it with `torch.Generator(\"cuda\").manual_seed([your_seed])`. Save the initials states with `generator.get_state()`, and update the generator with this initial state before each generation with the `set_state` method."
      ],
      "metadata": {
        "id": "_JDYPP_1VTeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a generator and save the initial state\n",
        "generator = torch.Generator(\"cuda\").manual_seed(1923936260)\n",
        "state = generator.get_state()\n",
        "\n",
        "# After each generation, reinitialize the generator to its initial state\n",
        "generator.set_state(state)"
      ],
      "metadata": {
        "id": "GTvFnO1dtLHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with height and width:"
      ],
      "metadata": {
        "id": "v-jBzdMZkbky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Create a prompt\n",
        "\n",
        "\n",
        "# CODE: Experiment with the height and width parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "G6gh3aouVZ9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with num_inference_steps:"
      ],
      "metadata": {
        "id": "fSWJF1ZJkYzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Create a prompt\n",
        "\n",
        "\n",
        "# CODE: Experiment with the num_inference_steps parameter.\n",
        "\n"
      ],
      "metadata": {
        "id": "PUx9fA81cJrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with guidance_scale. Generate several image for each value of the guidance scale, with different seed. What happen?"
      ],
      "metadata": {
        "id": "BRvux-JwkXX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Create a prompt\n",
        "\n",
        "\n",
        "# Experiment with the guidance_scale parameter:\n",
        "# CODE: Create two images with a low guidance scale without restarting the generator\n",
        "\n",
        "# CODE: Display both images using image_grid\n",
        "\n",
        "\n",
        "# CODE: Create two images with a strong guidance scale without restarting the generator\n",
        "\n",
        "\n",
        "# CODE: Display both images using image_grid\n",
        "\n"
      ],
      "metadata": {
        "id": "i_J8g9IicL2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with negative_prompt:\n"
      ],
      "metadata": {
        "id": "MRVg0u8FkUSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Create a prompt\n",
        "\n",
        "\n",
        "# Experiment with the negative_prompt parameter:\n",
        "# CODE: Create a first image without any negative prompt\n",
        "\n",
        "\n",
        "# CODE: Create a negative prompt, and generate an image with your original prompt and the negative prompt\n",
        "\n",
        "\n",
        "# CODE: Display both images using image_grid\n",
        "\n"
      ],
      "metadata": {
        "id": "Ngh_Z8pQcPW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53aZFyyCWEQU"
      },
      "source": [
        "## 2 - Implement you own diffusion model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, you only experimented with the pretrained weights of Stable Diffusion. But for real-case application, you may rather want to use a model trained on your own data.\n",
        "\n",
        "Several possibilities exists:\n",
        "- You can fine-tune Stable Diffusion 1.5 weights using [Low-Rank Adaptation](https://arxiv.org/abs/2106.09685) (Have a look [here](https://www.youtube.com/watch?v=70H03cv57-o) if you want to try that)\n",
        "- You can use the [ControlNet](https://arxiv.org/abs/2302.05543) architecture to control the generation with other modalities than just text or images. ControlNet prevent the [catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference) of that may happen when fine-tuning a deep learning model.\n",
        "- Or you can train a diffusion model from scratch! This is not really recommended, as diffusion models needs a lot of data and requires expensive compute. However, it is interesting to see how it can be done in practice, so we'll go for it.\n",
        "\n",
        "You have seen previously how to perform inference on a latent diffusion model. Here, you will implement a more standard diffusion model, with no control. The purpose of this model is to generate new sample from your target dataset.\n"
      ],
      "metadata": {
        "id": "3QjeR0SaVJ1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "Diffusion requires data. You can choose a dataset of your choice, but ensure that there are at least 1000 samples.\n",
        "\n",
        "You can check https://huggingface.co/huggan to take a dataset of your choice, e.g. [Smithsonian Butterflies Dataset](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) or [PokÃ©mon Image Dataset](https://huggingface.co/datasets/huggan/pokemon)."
      ],
      "metadata": {
        "id": "oOXnJXkjo7X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, start by loading your dataset using [pandas](https://pandas.pydata.org/):"
      ],
      "metadata": {
        "id": "xMV8RaLyshk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Choose the dataset you want to use\n",
        "# dataset_name = \"huggan/pokemon\"\n",
        "# dataset_name=\"huggan/smithsonian_butterflies_subset\"\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ],
      "metadata": {
        "id": "iGDderiorsiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with data, it is crucial to take a look at your data. Use the provided snippet to display a grid."
      ],
      "metadata": {
        "id": "Y_Xy2pewu92A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random_indices = torch.randint(0, len(dataset), (4,))\n",
        "\n",
        "def show_random_image(dataset):\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    for i, image in enumerate(dataset[random_indices][\"image\"]):\n",
        "        axs[i].imshow(image)\n",
        "        axs[i].set_axis_off()\n",
        "    fig.show()\n",
        "\n",
        "show_random_image(dataset)"
      ],
      "metadata": {
        "id": "W2ac9xNcU_QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may notice that all images are of different size. This is a common problem, but it can easily be solved by applying transformations to your data. You will also add some conventional data augmentation methods to artificially augment your dataset.\n",
        "\n",
        "Using the `Compose` class from `torchvision.transforms` package, create a composition with the transformations:\n",
        "- Resize images to size 128x128\n",
        "- Apply a RandomHorizontalFlip\n",
        "- Convert the images to tensors\n",
        "- Normalize the images to be in the range [-1, 1]"
      ],
      "metadata": {
        "id": "arU6NO5pvoD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Import the transform module from torchvision\n",
        "\n",
        "\n",
        "# CODE: Create an object named `preprocess` of class torch.Compose with the proper transformations.\n",
        "\n"
      ],
      "metadata": {
        "id": "1NKR-Z8dwjyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we defined a function to be used by the dataset object to transform the images\n",
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}"
      ],
      "metadata": {
        "id": "ZjyV6AmlucGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Use the `dataset`'s method `set_transform` to apply the preprocessing function to the images when needed (`on the fly`)\n",
        "\n"
      ],
      "metadata": {
        "id": "jfAGQMcyudkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize how the images have been modified\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset[random_indices][\"images\"]):\n",
        "    axs[i].imshow(image.cpu().detach().permute(1,2,0)*0.5+0.5)\n",
        "    axs[i].set_axis_off()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "r5ZDLGs80BJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, create your torch DataLoader for efficient training. You can use a batch size of 8 and activate shuffling."
      ],
      "metadata": {
        "id": "jU3FZeqj0Wyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Initialize the batch_size to 8\n",
        "\n",
        "\n",
        "# CODE: Create a train_dataloader with batch size 8 and shuffling activated.\n",
        "\n"
      ],
      "metadata": {
        "id": "LvOMAjI60Swx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the model"
      ],
      "metadata": {
        "id": "E43Jbemk04vI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this (small) diffusion model, you are going to use the same UNet as Stable Diffusion, but at a lower scale. The Diffusers library offers a simple integration of UNet with the class [UNet2DModel](https://huggingface.co/docs/diffusers/api/models/unet2d).\n",
        "\n",
        "The goal of the UNet is to evaluate the noise in an image. Therefore, as you are working in the explicit image space, it will take an image as input and output a tensor of the same dimension containing the estimated noise value for each channel of each pixel.\n",
        "\n",
        "The structure of a UNet is as follows:\n",
        "\n",
        "![UNet](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png)"
      ],
      "metadata": {
        "id": "acHPaMvp08me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize a UNet2DModel with the following configuration:\n",
        "- Set the sample size to match the size of your dataset's images.\n",
        "- Configure 3 input channels and 3 output channels, as we are working with RGB images.\n",
        "- Use 2 layers per block, resulting in 2 ResNet layers for each UNet block.\n",
        "- Set the block out channels as (128, 128, 256, 256, 512, 512).\n",
        "- Use 5 \"DownBlock2D\", the standard ResNet downsampling block. Insert a \"AttnDownBlock2D\" before the last \"DownBlock2D\".\n",
        "- Implement the symmetrical structure for the up part, using \"UpBlock2D\" and \"AttnUpBlock2D.\""
      ],
      "metadata": {
        "id": "-IZPSgfL1cXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Import UNet2DModel from the diffusers library\n",
        "\n",
        "\n",
        "# CODE: Initalize your model according to the guidelines.\n",
        "\n"
      ],
      "metadata": {
        "id": "BoM_dltw3qwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before executing the complete training loop, ensure that the input and output sizes of your model are the same:"
      ],
      "metadata": {
        "id": "EbH4a2rF4BS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image = dataset[0][\"images\"].unsqueeze(0)\n",
        "print(\"Input shape:\", sample_image.shape)\n",
        "\n",
        "print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)"
      ],
      "metadata": {
        "id": "-MEUCKF54i90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize your scheduler"
      ],
      "metadata": {
        "id": "gt1IA1yw4sI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [noise scheduler](https://huggingface.co/docs/diffusers/v0.23.1/en/api/schedulers/ddpm#diffusers.DDPMScheduler) handles the addition of noise during training, and the generation process during inference.\n",
        "\n",
        "Create a DDPMScheduler with 1000 timesteps."
      ],
      "metadata": {
        "id": "BcAjgw9_42X5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Import de DDPMScheduler\n",
        "\n",
        "\n",
        "# CODE: Initialize your noise scheduler\n",
        "\n"
      ],
      "metadata": {
        "id": "nFoGAhrs4o4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run the next cell to visualize how the noise degrades the data:"
      ],
      "metadata": {
        "id": "gZGuThI47jff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_noise_impact(dataset, noise_scheduler):\n",
        "\n",
        "    random_idx = torch.randint(0, len(dataset), (1,))\n",
        "\n",
        "    # Sample an image from the dataset\n",
        "    sample_image = dataset[random_idx][\"images\"][0]\n",
        "\n",
        "    # Sample random noise\n",
        "    noise = torch.randn(sample_image.shape)\n",
        "\n",
        "    # Select timesteps\n",
        "    timesteps = [50, 100, 500, 999]\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 4))\n",
        "    for i, t in enumerate(timesteps):\n",
        "        # Add noise to the image using the scheduler\n",
        "        noisy_image = noise_scheduler.add_noise(sample_image, noise, torch.tensor(t))\n",
        "        plt.subplot(1, len(timesteps), i+1)\n",
        "        plt.title(f\"t={t}\")\n",
        "        plt.imshow(noisy_image.permute(1,2,0)*0.5+0.5)\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "show_noise_impact(dataset, noise_scheduler)"
      ],
      "metadata": {
        "id": "56NhZyaJ7itB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train your model!"
      ],
      "metadata": {
        "id": "B4b0P8t__nzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are now only a few steps away from training your own diffusion model.\n",
        "\n",
        "Remember, the following step to prepare the training of a model with PyTorch are almost always the same:\n",
        "\n",
        "- Choose your training parameters. You can use\n",
        "- Define you loss function. Diffusion models are trained to estimate the noise added to an image. Thus, a simple mean square error loss can be used!\n",
        "- Define your optimizer. You can use AdamW, with a learning rate of 1e-4.\n",
        "- You can sometimes defines a learning rate scheduler. We recommand using the [Cosine Scheduler with Warmup](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html) with 500 learning rate warmup steps, and (len(dataloader) * n_epochs) as `num_training_steps`"
      ],
      "metadata": {
        "id": "SgYG4SkBBPfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE: Import everything you need: AdamW and MSELoss from PyTorch, and get_cosine_schedule_with_warmup from diffusers.\n",
        "\n",
        "\n",
        "# CODE: Defines the useful variable\n",
        "\n",
        "\n",
        "# CODE: Instantiate the loss function\n",
        "\n",
        "\n",
        "# CODE: Defines your optimizer\n",
        "\n",
        "\n",
        "# CODE: Create the learning rate scheduler\n",
        "\n"
      ],
      "metadata": {
        "id": "usU1iIQ6BMF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then write your training loop. Iterates over your dataset n_epochs times. At each step:\n",
        "- Start by clearing all gradients using the zero_grad method of your optimizer\n",
        "- Sample a random noise of the same shape as your batch (use `torch.randn`)\n",
        "- Sample a random timestep for each image (use `torch.randint`)\n",
        "- Add the sampled noise to your image using the noise scheduler\n",
        "- Feed the noisy batch to your model, and get the estimated noise (Forward pass)\n",
        "- Compute the MSE between the noise you sampled and estimation\n",
        "- Run the backward pass.\n",
        "- Update your optimizer\n",
        "- Update your learning rate scheduler\n",
        "\n",
        "After each epoch, use the provided `evaluate` function to generate images"
      ],
      "metadata": {
        "id": "H3McZbNVDSd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers.utils import make_image_grid\n",
        "import os\n",
        "\n",
        "def evaluate(epoch, pipeline, batch_size=4):\n",
        "    # Sample some images from random noise (this is the backward diffusion process).\n",
        "    # The default pipeline output type is `List[PIL.Image]`\n",
        "    images = pipeline(\n",
        "        batch_size=batch_size,\n",
        "    ).images\n",
        "\n",
        "    # Make a grid out of the images\n",
        "    image_grid = make_image_grid(images, rows=2, cols=2)\n",
        "\n",
        "    display(image_grid)"
      ],
      "metadata": {
        "id": "jsYx9w7vK-OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from tqdm import tqdm\n",
        "from diffusers import DDPMPipeline\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "evaluate_every_n_epochs = 1\n",
        "\n",
        "losses = []\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
        "        clean_images = batch[\"images\"].to(device)\n",
        "\n",
        "        # CODE: Clear all gradients\n",
        "\n",
        "        # CODE: Sample random noise\n",
        "\n",
        "        # CODE: Sample a random timestep for each image\n",
        "\n",
        "\n",
        "        # CODE: Use the noise_scheduler to add noise to your image\n",
        "\n",
        "        # CODE: Run the forward pass. Note: pass return_dict=False to your model.\n",
        "\n",
        "        # CODE: Evaluate the loss\n",
        "\n",
        "        # CODE: Run the backward pass\n",
        "\n",
        "        # CODE: Perform the optimizer and scheduler steps\n",
        "\n",
        "        # Aggregate the losses\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch} / Epoch Loss {epoch_loss / len(train_dataloader)}\")\n",
        "    losses.append(epoch_loss / len(train_dataloader))\n",
        "\n",
        "    # We now create a pipeline and evaluate the model!\n",
        "    pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
        "    if (epoch + 1) % evaluate_every_n_epochs == 0:\n",
        "        evaluate(epoch, pipeline)\n",
        "\n",
        "# Plot the final results\n",
        "plt.plot(range(len(losses)), losses)"
      ],
      "metadata": {
        "id": "ww2PHJIvBLMW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}